*********************
Informatics Overview
*********************

.. note:: 
	This document is part of the OICR Genomics Quality Management System : TM-014 Informatics Pipelines

The informatics pipeline consists of a combination of automated production software and manually executed scripts. A diagram illustrating the flow of data through individual components is shown in :numref:`high-level-overview`. More information about specific systems is available below.

This SOP describes steps 1 through 5. Steps 6 and 7 are described in TM. Data Review and Reporting Procedure.

1. The informatics pipeline commences once a sequencing run is complete, at which point Run Scanner registers the run as complete, and the laboratory information management system (LIMS) - MISO retrieves and stores this information. 
2. An automation and decision-making software Shesmu retrieves the status of completed runs from MISO’s API (Pinery), and in turn triggers the launching of bcl2fastq for generation of raw FASTQ files. 
3. Shesmu also interfaces with the provenance client - a series of files generated by the provenance client containing information about completed workflow runs in order to generate the instructions for launching the complete bioinformatics pipeline. 
4. After bcl2fastq is complete, subsequent bioinformatics analysis pipelines are launched by Vidarr. More details about the analysis pipelines is available in each assay_.
5. Once analysis is completed, ETL is responsible for retrieving, and subsequently transforming workflows with quality control information for display via Dashi and Dimsum. 
6. Additionally, an in-house application named Djerba (https://github.com/oicr-gsi/djerba) retrieves and transforms data containing mutation, copy number, structural variation, and expression calls into a tabular format for review by Cancer Genome Interpreters. This information is subsequently processed by Djerba to generate a PDF clinical report.
7. The final step in the process is digital signing and upload of the PDF to the sample requisition system.

.. _assay: reference.html

.. _high-level-overview:

.. figure:: images/high-level-overview.png

   **High-level overview of the informatics pipeline architecture.** The clinician or their representative submits a requisition with information about a patient and their clinical metadata (non-PHI) which is approved by laboratory managers in the Requisition system. The requisition ID is transferred by Tissue Portal to associate a specific set of Tissues with the requisition in MISO LIMS. After sequencing is completed, the completed run and its configuration and quality metrics are registered by Run Scanner. This information is pulled into MISO LIMS, and the Run appears in the interface for laboratory personnel to attach to pool information (a pool of libraries). MISO exposes this information via its API, Pinery. Shesmu retrieves the information about the libraries and the sequencing run and uses it to launch a series of pipeline steps, which produce raw pipeline output. This output is registered in Provenance and used by GSI-QC-ETL to display quality control information in Dashi and Dimsum. This output is further used in combination with information from the Requisition system, MISO, Provenance, and ETL by Djerba to produce a preliminary clinical report for review. The preliminary report is checked and refined by an Analyst, who then uses additional functions of Djerba to create the unsigned report ready for the Geneticist to review. The signed report is then attached to the original requisition in the Requisition system for return to the clinician.  


***************************
Infrastructure Components
***************************

Run Scanner (1.12.0+)
	Run Scanner is a Java application which monitors sequencing run status http://runscanner.gsi.oicr.on.ca internal only) and is used to populate and update runs in the laboratory information management system (LIMS) – MISO, including marking them as complete.

MISO (1.14+)
	MISO is the in-house developed and maintained open-source LIMS software. MISO (https://miso.oicr.on.ca) is a Java application responsible for tracking all samples and the associated metadata from initial sample accessioning through to drafting of reports and delivery of data. MISO functionality is based on the concept of sample derivation and propagation as a sample moves through the laboratory lifecycle. The hierarchy of metadata is as follows: 

	Identity (donor)→Tissue →Sample → Library → Library Aliquot, where an arrow represents a parent→child relationship. 

	Library aliquots are then placed into sequencing pools which are customized according to the specifications of each individual sequencing unit (ie. Illumina flow cells and lanes). Propagation through the hierarchy of levels is strictly enforced by the user interface, and unique identifiers are automatically generated and recorded for each level in a SQL database. Multiple or subsequent samples received for a given donor would automatically generate incremented identifiers allowing cross-referencing between samples. These unique identifiers allow for automatic association of downstream bioinformatics data to the specific library, sample, tissue, and donor. The use of MISO is described in more detail in TM. LIMS Usage - MISO.

Provenance records
	Provenance records connect run, sample, and analysis metadata, therefore allowing one to trace all analysis for a given sample. The provenance records consist of three types of information. Note that each system is described in more detail below. 
	The three types of provenance include:

	1.	Sample provenance, provided by Pinery, which contains the information about samples captured in the MISO database.
	2.	Analysis provenance, provided by Vidarr, containing information regarding analysis metadata and files generated from the informatics pipeline workflows. 
	3.	File provenance, generated as needed by the Provenance Client, combines LIMS and Analysis provenance to show the provenance (history and origin) of each file produced in analysis.

	Each type of provenance has a unique key that allows it to be joined together and split apart. 

Pinery (2.13.0+)
	Pinery (http://pinery.gsi.oicr.on.ca internal network access only) is a webservice which exposes the MISO data as an API to retrieve information about samples, libraries, runs, and other information recorded in the LIMS. Both file provenance and Shesmu (described below) use this system as a primary source of information.

Requisition system
	The requisition system (https://requisition.genomics.oicr.on.ca/) is an external facing web application sample accessioning system for external collaborators submitting clinical samples to OICR. Details of the requisition system, including current version, are described in the QM. Requisition and Reporting System SOP. The requisition system contains all of the clinical metadata associated with samples submitted for clinical sequencing, including automatically generated identifiers, which are transferred into MISO by trained laboratory technicians (please refer to the TM. Sample Accessioning Procedure SOP).

Provenance Client (2.5.17+)
	The Provenance Client is a command line application and library written in Java which retrieves sample provenance from Pinery and analysis provenance from Vidarr and combines it into file provenance. The library is also used directly by Shesmu, which is used to automate subsequent workflows and tasks.

Shesmu (1.4.2+) and Vidarr (2.0.3)
	Analysis is automated through decision-making software developed in-house called Shesmu (https://shesmu.gsi.oicr.on.ca/- internal network access only). Shesmu acts as an intermediary between several systems: MISO, a database of analysis provenance, and a workflow scheduler. Shesmu interfaces with MISO to retrieve sample metadata, and scans the provenance system for a list of which files have been produced and then uses decision-action blocks called “Olives” to decide what “actions” should be run. Actions can be launching analysis workflows, filing tickets in JIRA, generating reports, updating QC data, notifying operators about invalid data, requesting the laboratory technicians enter missing required data, and informing the lab of the current analysis progress. All production tasks pertaining to the monitoring and configuration of the informatics pipeline is performed via version-controlled Olives.

	Vidarr is an analysis provenance tracking server. It schedules workflows using a workflow engine like Cromwell workflow execution engine, collects the output from these workflows, and stores metadata about files and connections to Pinery LIMS information. Its primary components include a web service to track analysis, a command line interface for testing and development, and a base workflow engine (Cromwell), as well as tools for generating workflow definitions.

	Workflows are written in the WDL language and contain all the commands for running bioinformatics software (including fastq generation, alignment, variant calling, annotation and generation of QC metrics; see :numref:`wgs-pipeline` and :numref:`wts-pipeline` for flowchart of WGS and RNA bioinformatics workflows, respectively. See next section for details of the software components within WDLworkflows). Workflow runs and all associated files are tracked and recorded in Vidarr. This information is exposed as analysis provenance and used by the Provenance Client and Shesmu for automation.

Modulator (0.1)
	Modulator is a Python script for automatically building environment modules in the cluster environment (https://gitlab.oicr.on.ca/ResearchIT/modulator). Modulator reads .yaml configuration files containing the build “recipe”: a configuration of instructions which calls various functions for downloading and building bioinformatics software. Access to the resulting modules are controlled at the group level, allowing only users within the appropriate group the ability to load modules, including the production users ‘hsqwprod’ and ‘seqprodbio’. All recipes are version controlled, thus allowing for lockdown of the modules used for the production pipeline.

ETL/Dashi (200824-1630)
	Dashi (https://dashi.oicr.on.ca/) is an in-house dashboard based on Python’s Dash framework for visualizing sample QC metrics. GSI-QC-ETL (ETL) is a series of data handler scripts which ingests the output from QC workflow runs and formats the data into a tabular format amenable for plotting via Dashi. ETL scripts and Dashi updates are configured to run automatically via Olives as new data is processed.

Dimsum
	Dimsum (https://dimsum.gsi.oicr.on.ca internal network access only) is an in-house developed dashboard that presents data from MISO, GSI-QC-ETL, and other OICR systems with the goal of streamlining quality control and other processes.

Djerba
	Djerba (https://github.com/oicr-gsi/djerba) is an in-house application used by CGI interpreters to create WGTS, pWGS and TAR reports and facilitate interpretation.

Atlassian JIRA (8.7.1+)
	JIRA is a ticketing system used by Genomics to alert on issues that require human intervention. It is not directly used in analysis. Its use is further described in QM. LIMS Issue Management Plan.

Grafana (6.7.0+)
	Grafana is a graph-based monitoring tool used by Genomics to show trends in performance over time. It is not directly used in analysis.


********************
Updates and Upgrades
********************

Although every effort is made to ensure that the processes laid out in these QMS documents are complete and correct, software must occasionally be updated to support new features, changes in system integration or bug fixes. 

With the exception of the software detailed below, software and pipelines that support or underpin accredited assays. Any research-related changes will not trigger an update process. Production configuration is kept separately; software installed for production is also kept separately; and informatics pipelines are installed specifically for clinically-reported assays (ACD/CAP).

Several mechanisms can trigger changes to software: 

1.	User-submitted bug reports and tickets, submitted and prioritized as described in QM. LIMS Issue Management Plan;
2.	Notifications from Research IT. Research IT regularly scans all OICR-operated software looking for vulnerabilities and will notify us of any issues.
3.	Errors during informatics pipeline execution may also trigger bug reports as in #1. Workflows contain self-checking steps that ensure that results are consistent.


General Procedure
==================

All informatics pipelines and infrastructure follow software engineering best practices for software development. The following is the general procedure followed by every software used in production.

1.	Changes are made to a copy of the software, saved in a version-controlled ‘branch’ in the software repository.
2.	The developer issues a "pull request" (change request) that describes what is being changed and why.
3.	The changes are peer reviewed and signed off by at least two reviewers. This sign-off is dated and versioned with the signatory’s name and kept in perpetuity.
4.	The changes are merged in with the software repository.
5.	If an SOP change is required (including but not limited to this SOP, QM. Quality Control and Calibration Procedures or TM. Data Review and Reporting), a QW. Software Update Form should be filed by the change requestor.
6.	The changes are deployed to the development and staging environments for testing and evaluation.
7.	If a Software Update form is required, deployment to production must await validation (if necessary) and/or sign-off on the Software Update Form.
8.	Once the software is performing as expected in those environments, we deploy changes to production
9.	Through the use of monitoring, logging, and change history we are able to identify when updates and upgrades are not performing as expected. Because everything is versioned, we are able to ‘roll back’ changes if necessary.

Any significant changes made to software that impact procedures in the QMS trigger updates to SOPs using the QM. Document Control Plan.

MISO LIMS
=========

MISO LIMS is actively developed to support new data types, user feature requests, bug fixes and security updates. One deployment is used by both research and accredited laboratory processes. MISO follows a scheduled release cycle in which all changes since the last release are combined and released together. Note that Software Update Forms are not required for MISO releases.

Changes to the MISO code base proceed as follows:

1.	MISO is updated, a pull request is made, and reviewed as in the general procedure.
2.	The changes are automatically tested using continuous integration to ensure no unwanted behaviour is introduced and that previously found regressions are not reintroduced. 
3.	If both review and continuous integration pass, the changes are merged to the repository. 
4.	A detailed release procedure is used for deployment to staging and production environments (https://wiki.oicr.on.ca/display/MISO/Release+Procedure). It involves manually testing the new features, ensuring that the LIMS provenance data exported to analysis has not changed unexpectedly or in an incompatible way and MISO integration with other systems is not broken. 
5.	Once the update is available in staging, changes to MISO are communicated to users via email and users are given time to preview and test features in staging. 
6.	After the preview period, the release to production is then performed at the scheduled time, users are given a demo of new features in their lab meeting, and then are notified that they may resume using MISO production.

Dashi
=====

Dashi is actively developed to support new report types, metrics, bug fixes and security updates. One Dashi instance (https://dashi.oicr.on.ca) supports both accredited processes and research processes, so it is updated regularly. Dashi follows a scheduled release cycle in which all changes since the last release are combined together. Note that Software Update Forms are not required for Dashi releases.

Changes to Dashi proceed as follows:

1.	Dashi is updated, a pull request is made, reviewed, and merged as in the general procedure.
2.	A detailed release procedure is used for deployment to staging and production environments (https://wiki.oicr.on.ca/pages/viewpage.action?pageId=137528978). It involves reviewing that input metrics data can be loaded and reports can be generated. 
3.	Once the updates are available in staging, changes to Dashi are communicated to users via Slack and users are given time to preview and test features in staging.
4.	After the preview period, release to production is performed at the scheduled time and users are given a demo of the new features.

Dimsum
=======

Dimsum is actively developed to support new features, metrics, bug fixes, and security updates. One Dimsum instance (https://dimsum.gsi.oicr.on.ca) supports both accredited and research processes, so it is updated regularly. Dimsum follows a scheduled release cycle in which all changes since the last release are combined together. Note that Software Update Forms are not required for Dimsum releases,

Changes to Dimsum proceed as follows:

1.	Dimsum is updated, a pull request is made, reviewed, and merged as in the general procedure.
2.	A detailed release procedure is used for deployment to staging and production environments (https://wiki.oicr.on.ca/x/hAOSD)
3.	Once the updates are available in staging, changes to Dimsum are communicated to users via email and users are given time to preview and test features in staging.
4.	After the preview period, release to production is performed at the scheduled time and users are given a demo of the new features.

Pipeline and infrastructure configuration
=========================================

Production configuration determines which servers run the software, perform backups, and log statuses, and also determine how the pipeline fits together. The vast majority of changes to configuration and infrastructure will not impact analysis or the final report, and follow the General Procedure detailed above. On the occasions where changes will impact analysis and/or change SOPs, a Software Update form must be filled in, and validation may be required before it can be deployed. All changes regardless of impact are recorded in the commit log at https://bitbucket.oicr.on.ca/projects/GSI/repos/analysis-config/commits for pipeline configuration, and https://bitbucket.oicr.on.ca/projects/GSI/repos/infrastructure/commits for infrastructure.

Pipeline software components and workflows
===========================================

Occasionally, the informatics pipelines themselves will require updates, although these are specific to the accredited informatics pipelines and so these updates are rare. They also follow the same general procedure for upgrades detailed above. 

1.	The informatics pipeline is updated, a pull request is made, and reviewed.
2.	Changes in the pull request are automatically tested using continuous integration to ensure that the output from the analysis workflow remains unchanged (or that the change in output is expected).
3.	If both review and continuous integration pass, the changes are merged to the repository.
4.	Analysis workflow updates follow a release procedure from development, through testing (staging), and thorough to production (https://wiki.oicr.on.ca/display/GSI/Workflow+and+Olive+Release+Checklist).
5.	If analysis output is unchanged (in the case of security patches or other updates that will not change the output), pipelines are updated to the latest analysis workflow version.  
6.	If the pipeline changes the analysis results, the pipeline is rerun with validation samples to confirm that the analysis results are as expected. These changes are then documented in the relevant SOPs and the change is released into production.

*************************************
Variables and Observations to Record
*************************************

Quality control metrics are reviewed as described in the QM. Quality Control and Calibration Procedures SOP.

Procedure
==========

The informatics pipeline is automatic and controlled by the decision-making software, Shesmu. The following steps describe the roles of pipeline leads, whose responsibility is general oversight of all aspects of pipeline automation, as well as the Cancer Genome Interpreters who depend on completion of the pipeline. In general, the procedure for pipeline leads consists of monitoring workflow run statuses in Shesmu, and acting when there is an error. While pipeline leads are primarily responsible, Cancer Genome Interpreters may also monitor workflow run statuses and file JIRA tickets for the pipeline leads to follow-up.

The automated actions of the pipelines are described in subsequent sections for the specific analysis pipelines.


Monitoring pipeline
===================

1.	Once a sequencing run is marked in MISO as complete, an e-mail is automatically sent to gsi-qc@lists.oicr.on.ca, indicating that the run is complete to review on sequencer metrics (the procedure for lab personnel reviewing run QC metrics is described in QC SOP).
2.	Pipeline leads monitor the automation system and analysis runs depicted in :numref:`shesmu-pipeline-lead`. The full procedure for monitoring the pipeline status and resolving common issues is listed in the Pipeline Lead Role documentation. In short, pipeline leads 1) watch for automated alerts (email, JIRA, and Slack) from the software infrastructure indicating problem with computers or upstream data; 2) check the Pipeline Lead Dashboard in the Shesmu UI for any errors related to analysis runs; 3) check Grafana for the presence of ERROR or STALE records in provenance, which indicates that a change was made in LIMS to files that were previously analyzed. Pipeline leads will attempt to resolve any issues they encounter that are transient and related to software or hardware failures (common in high-throughput computing environments) or will contact CGI for any issues arising from data analysis.
3.	Cancer Genome Interpreters may also monitor the status of workflow runs and file tickets for follow-up from the pipeline lead. The following is an example of monitoring the bcl2fastq olive, which is the first olive launched upon completion of a sequencing run :numref:`shesmu-pipeline-lead`:

	a.	Pipeline leads navigate to Shesmu UI to monitor workflow run statuses.
	b.	Workflows in the “HALP” or “FAILED” state are investigated further by clicking “Drill Down” which is displayed after clicking on the workflow status.
	c.	Actions that require immediate follow-up by pipeline leads and be ticketed in JIRA by referencing the shesmuID for individual actions.

4.	Once all pipelines are completed, Clinical Genome Interpreters proceed with the TM. Data Review and Reporting SOP.



.. image:: images/shesmu1.png

.. _shesmu-pipeline-lead:

.. figure:: images/shesmu2.png

	**Representation of procedure for monitoring workflow run statuses:** Checking for failed workflow runs and filing a JIRA ticket for pipeline leads to follow up.


The entire pipeline is automated and, in the ideal case, requires no human intervention.


